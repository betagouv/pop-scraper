# POP Scraper

Aspirateur de donnÃ©es de la [Plateforme Ouverte du Patrimoine](https://www.pop.culture.gouv.fr).

DÃ©veloppÃ© dans le cadre de [Collectif Objets](https://collectif-objets.beta.gouv.fr/)

CodÃ© en Python 3 ðŸ avec la librairie [Scrapy](https://docs.scrapy.org/)

## Production

DÃ©ployÃ© et disponible sur [zyte.com](https://app.zyte.com/)

## DÃ©veloppement local

- `poetry install`
- `poetry run scrapy crawl pop_api`

## Options

### `base_pop`

Base POP Ã  scrapper. Seules `memoire`, `palissy` et `merimee` sont supportÃ©es. DÃ©fault `palissy`

`poetry run scrapy crawl pop_api -a base_pop=memoire`

### `max_items`

Limite le nombre d'objets Ã  parcourir

`poetry run scrapy crawl pop_api -a max_items=200`

### `ref`

Scrappe un seul objet grace Ã  sa rÃ©fÃ©rence Palissy

`poetry run scrapy crawl pop_api -a ref=PM72000741`

## Scripts

- `make split_csv_by_dpt csv_path=exports/palissy.csv`


## Rajouter une base POP (Joconde, MNR etc)

- rÃ©cupÃ©rer un item en JSON depuis le navigateur sur la page de recherche
- rÃ©cupÃ©rer le CSV de description des champs de la base dans la codebase de POP
- rajouter la classe d'Item dans items.py Ã  partir de ces deux fichiers comparÃ©s - la source de vÃ©ritÃ© est ce qu'on peut rÃ©cupÃ©rer depuis l'API de recherche
- faire un premier import puis pour chaque array, vÃ©rifier qu'il y a vraiment des valeurs multiples avec `select count(*) from palissy where DENO is not null and json_array_length(DENO) > 1;` par exemple. sinon, changer le serializer
