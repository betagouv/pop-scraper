# POP Scraper

> [!WARNING]
> Les donnÃ©es de POP sont en train dâ€™Ãªtre publiÃ©es sur [data.culture.gouv.fr](https://data.culture.gouv.fr/), ce qui rendra ce scraper obsolÃ¨te.

Aspirateur de donnÃ©es de la [Plateforme Ouverte du Patrimoine](https://www.pop.culture.gouv.fr).

DÃ©veloppÃ© dans le cadre de [Collectif Objets](https://collectif-objets.beta.gouv.fr/)

CodÃ© en Python 3 ðŸ avec la librairie [Scrapy](https://docs.scrapy.org/)

## Production

DÃ©ployÃ© et disponible sur [zyte.com](https://app.zyte.com/)

## DÃ©veloppement local

- `poetry install`
- `poetry run scrapy crawl pop_api`

## Options

### `base_pop`

Base POP Ã  scrapper. Seules `palissy` et `merimee` sont supportÃ©es. DÃ©fault `palissy`

`poetry run scrapy crawl pop_api -a base_pop=merimee`

### `max_items`

Limite le nombre d'objets Ã  parcourir

`poetry run scrapy crawl pop_api -a max_items=200`


## Rajouter une base POP (Joconde, MNR etc)

- rÃ©cupÃ©rer un item en JSON depuis le navigateur sur la page de recherche
- rÃ©cupÃ©rer le CSV de description des champs de la base dans la codebase de POP
- rajouter la classe d'Item dans items.py Ã  partir de ces deux fichiers comparÃ©s - la source de vÃ©ritÃ© est ce qu'on peut rÃ©cupÃ©rer depuis l'API de recherche
- faire un premier import puis pour chaque array, vÃ©rifier qu'il y a vraiment des valeurs multiples avec `select count(*) from palissy where DENO is not null and json_array_length(DENO) > 1;` par exemple. sinon, changer le serializer

## Useful commands

use `watch` and `xsv` to follow the number of lines in the CSV outputs during the crawl as it grows
(wc -l does not work because csv rows can contain new lines)

`watch -n 10 "find tmp2 -name '*.csv' -exec sh -c 'echo -n "{}: "; xsv count {}' \;"`
